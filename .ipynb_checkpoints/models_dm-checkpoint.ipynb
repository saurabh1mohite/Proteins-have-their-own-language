{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47606\n",
      "15869\n"
     ]
    }
   ],
   "source": [
    "DIR = './data/sequences/dm/'\n",
    "import pickle\n",
    "\n",
    "with open(DIR + 'train/regressors_dm.pkl', 'rb') as fp:\n",
    "    X_train = pickle.load(fp)\n",
    "print(len(X_train))\n",
    "with open(DIR + 'test/regressors_dm.pkl', 'rb') as fp:\n",
    "    X_test = pickle.load(fp)\n",
    "print(len(X_test))\n",
    "with open(DIR + 'train/targets_dm.pkl', 'rb') as fp:\n",
    "    y_train = pickle.load(fp)\n",
    "\n",
    "with open(DIR + 'test/targets_dm.pkl', 'rb') as fp:\n",
    "    y_test = pickle.load(fp)\n",
    "pivot = len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "scaler = StandardScaler()\n",
    "from tqdm import tqdm_notebook\n",
    "X = X_train + X_test\n",
    "y = y_train + y_test\n",
    "X = scaler.fit_transform(X)\n",
    "X_train = X[:pivot]\n",
    "X_test = X[pivot:]\n",
    "y_train = y[:pivot]\n",
    "y_test = y[pivot:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47606 47606 15869 15869\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.5320436070325792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf1 = ExtraTreesClassifier(n_estimators=100, max_features=\"auto\",random_state=0).fit(X_train, y_train)\n",
    "print(clf1.score(X_train, y_train), clf1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/document_embedding_based_models/extra_trees_dm.pkl', 'wb') as fp:\n",
    "    pickle.dump(clf1, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38518254001596436 0.38590963513768983\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf2 = GaussianNB().fit(X_train, y_train)\n",
    "print(clf2.score(X_train, y_train), clf2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/document_embedding_based_models/GaussianNB_dm.pkl', 'wb') as fp:\n",
    "    pickle.dump(clf2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.605364869974373 0.4711702060621337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf3 = LogisticRegression(random_state=0, max_iter=40000).fit(X_train, y_train)\n",
    "print(clf3.score(X_train, y_train), clf3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/document_embedding_based_models/logreg_dm.pkl', 'wb') as fp:\n",
    "    pickle.dump(clf3, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7764777549048439 0.47967735837166803\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf4 = SVC(kernel='linear').fit(X_train, y_train)\n",
    "print(clf4.score(X_train, y_train), clf4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/document_embedding_based_models/svm_dm.pkl', 'wb') as fp:\n",
    "    pickle.dump(clf4, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.5375259940765014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf5 = RandomForestClassifier(n_estimators=100, max_features=\"auto\",random_state=0).fit(X_train, y_train)\n",
    "print(clf5.score(X_train, y_train), clf5.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/document_embedding_based_models/random_forest_dm.pkl', 'wb') as fp:\n",
    "    pickle.dump(clf5, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.5794946121368706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf6 = KNeighborsClassifier(n_neighbors= 7, weights='distance', algorithm='auto', metric='euclidean', n_jobs=-1).fit(X_train, y_train)\n",
    "print(clf6.score(X_train, y_train), clf6.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/document_embedding_based_models/knn_dm.pkl', 'wb') as fp:\n",
    "    pickle.dump(clf6, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7350544049069445 0.6219673577415086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf7 = SVC(kernel='rbf').fit(X_train, y_train)\n",
    "print(clf7.score(X_train, y_train), clf7.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/document_embedding_based_models/svm_rbf_dm.pkl', 'wb') as fp:\n",
    "    pickle.dump(clf7, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEHE END !!\n"
     ]
    }
   ],
   "source": [
    "print('HEHE END !!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'clf1' : clf1,\n",
    "    'clf2' : clf2,\n",
    "    'clf3' : clf3,\n",
    "    'clf4' : clf4,\n",
    "    'clf5' : clf5,\n",
    "    'clf6' : clf6,\n",
    "    'clf7' : clf7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf1.score(X, y))\n",
    "print(clf2.score(X, y))\n",
    "print(clf3.score(X, y))\n",
    "print(clf4.score(X, y))\n",
    "print(clf5.score(X, y))\n",
    "print(clf6.score(X, y))\n",
    "print(clf7.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "temp_y = []\n",
    "for i in y_train:\n",
    "    if i.endswith('\\n'):\n",
    "        temp_y.append(i[:-1])\n",
    "    else:\n",
    "        temp_y.append(i)\n",
    "y_train = temp_y\n",
    "\n",
    "\n",
    "temp_y = []\n",
    "for i in y_test:\n",
    "    if i.endswith('\\n'):\n",
    "        temp_y.append(i[:-1])\n",
    "    else:\n",
    "        temp_y.append(i)\n",
    "y_test = temp_y\n",
    "y = y_train + y_test\n",
    "\n",
    "len(list(set(y_train)))\n",
    "\n",
    "\n",
    "temp = list(set(y))\n",
    "with open ('./data/document_embedding_based_models/label2idx.pkl', 'rb') as pkl:\n",
    "    d = pickle.load(pkl)\n",
    "temp_y = []\n",
    "for i in y:\n",
    "    temp_y.append(d[i])\n",
    "y = temp_y\n",
    "y[0]\n",
    "y_true = y\n",
    "\n",
    "for i in range(1, 8):\n",
    "    y_preds = classifiers['clf'+str(i)].predict(X)\n",
    "    temp = []\n",
    "    for j in y_preds:\n",
    "        if j.endswith('\\n'):\n",
    "            temp.append(j[:-1])\n",
    "        else:\n",
    "            temp.append(j)\n",
    "    y_preds = [d[j] for j in temp]\n",
    "    cm = confusion_matrix(y_true, y_preds)\n",
    "    print(i)\n",
    "    plt.figure(figsize=(40, 40))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Pastel1')\n",
    "    plt.title('Confusion matrix', size = 36)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(36)\n",
    "    plt.xticks(tick_marks, [i for i in list(set(y_true))], rotation=90, size = 15)\n",
    "    plt.yticks(tick_marks, [i for i in list(set(y_true))], size = 15)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Actual label', size = 36)\n",
    "    plt.xlabel('Predicted label', size = 36)\n",
    "    width, height = cm.shape\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(cm[x][y]), xy=(y, x), horizontalalignment='center', verticalalignment='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "methyltransferase superfamil\n",
      "38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(y_train[0])\n",
    "temp_y = []\n",
    "for i in y_train:\n",
    "    if i.endswith('\\n'):\n",
    "        temp_y.append(i[:-1])\n",
    "    else:\n",
    "        temp_y.append(i)\n",
    "y_train = temp_y\n",
    "\n",
    "\n",
    "temp_y = []\n",
    "for i in y_test:\n",
    "    if i.endswith('\\n'):\n",
    "        temp_y.append(i[:-1])\n",
    "    else:\n",
    "        temp_y.append(i)\n",
    "y_test = temp_y\n",
    "y = y_train + y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "temp = list(set(y))\n",
    "with open ('./data/document_embedding_based_models/label2idx.pkl', 'rb') as pkl:\n",
    "    d = pickle.load(pkl)\n",
    "temp_y = []\n",
    "for i in y:\n",
    "    temp_y.append(d[i])\n",
    "y = temp_y\n",
    "y_train = [d[i] for i in y_train]\n",
    "y_test = [d[i] for i in y_test]\n",
    "y[0]\n",
    "y_true = y\n",
    "print(y_train[0])\n",
    "len(list(set(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(list(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = label_binarize(np.array(y_train), classes=list(set(y_train)))\n",
    "y_test = label_binarize(np.array(y_test), classes=list(set(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = y_train.shape[1]\n",
    "n_samples, n_features = X.shape\n",
    "import os\n",
    "def plot_roc(clf, file, linestyle):\n",
    "    classifier = OneVsRestClassifier(clf)\n",
    "    if os.path.exists('./data/document_embedding_based_models/' + file + '.pkl'):\n",
    "        with open ('./data/document_embedding_based_models/' + file + '.pkl', 'rb') as pkl:\n",
    "            model = pickle.load(pkl)\n",
    "    else:\n",
    "        model = classifier.fit(X_train, y_train)\n",
    "        with open ('./data/document_embedding_based_models/' + file + '.pkl', 'wb') as pkl:\n",
    "            pickle.dump(model, pkl)\n",
    "    if file.endswith('linear') or file.endswith('rbf'):\n",
    "        y_score = model.decision_function(X_test)\n",
    "    else:\n",
    "        y_score = model.predict_proba(X_test)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    lw = 2\n",
    "\n",
    "    \n",
    "    print(file)\n",
    "#     print(model.score(X_train, y_train), model.score(X_test, y_test), model.score(X, y))\n",
    "    plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "    plt.rcParams['xtick.labelsize']= 12\n",
    "    plt.rcParams['ytick.labelsize'] = 12\n",
    "    plt.plot(fpr[2], tpr[2],\n",
    "             lw=lw, label=file + ' (area = %0.2f)' % roc_auc[2], color='black', linestyle=linestyle)\n",
    "    plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', size=14)\n",
    "    plt.ylabel('True Positive Rate', size=14)\n",
    "    plt.title('Receiver operating characteristic Plot on DM Data', size=16)\n",
    "    plt.legend(loc=\"lower right\", fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "linestyles = OrderedDict(\n",
    "    [('solid',               (0, ())),\n",
    "     ('loosely dotted',      (0, (1, 10))),\n",
    "     ('dotted',              (0, (1, 5))),\n",
    "     ('densely dotted',      (0, (1, 1))),\n",
    "\n",
    "     ('loosely dashed',      (0, (5, 10))),\n",
    "     ('dashed',              (0, (5, 5))),\n",
    "     ('densely dashed',      (0, (5, 1))),\n",
    "\n",
    "     ('loosely dashdotted',  (0, (3, 10, 1, 10))),\n",
    "     ('dashdotted',          (0, (3, 5, 1, 5))),\n",
    "     ('densely dashdotted',  (0, (3, 1, 1, 1))),\n",
    "\n",
    "     ('loosely dashdotdotted', (0, (3, 10, 1, 10, 1, 10))),\n",
    "     ('dashdotdotted',         (0, (3, 5, 1, 5, 1, 5))),\n",
    "     ('densely dashdotdotted', (0, (3, 1, 1, 1, 1, 1)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM Extra Trees Classifier\n",
      "DM Gaussian Naive Bayes\n",
      "DM Logistic Regression\n",
      "DM Random Forest Classifier\n",
      "DM K-Nearest Neighbor\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "plot_roc(ExtraTreesClassifier(n_estimators=100, max_features=\"auto\",random_state=0), 'DM Extra Trees Classifier', linestyles['loosely dotted'])\n",
    "plot_roc(GaussianNB(), 'DM Gaussian Naive Bayes', linestyles['dotted'])\n",
    "plot_roc(LogisticRegression(random_state=0, max_iter=40000), 'DM Logistic Regression', linestyles['dashed'])\n",
    "plot_roc(RandomForestClassifier(n_estimators=100, max_features=\"auto\",random_state=0), 'DM Random Forest Classifier', linestyles['loosely dashdotted'])\n",
    "plot_roc(KNeighborsClassifier(n_neighbors= 7, weights='distance', algorithm='auto', metric='euclidean', n_jobs=-1), 'DM K-Nearest Neighbor', linestyles['densely dashdotted'])\n",
    "plot_roc(SVC(kernel='linear'), 'DM SVM kernel=linear', linestyles['densely dashdotdotted'])\n",
    "plot_roc(SVC(kernel='rbf'), 'DM SVM kernel=rbf', linestyles['solid'])\n",
    "plt.savefig(os.path.join('./data/document_embedding_based_models', 'dm_roc.tif'), dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "files = ['DM Gaussian Naive Bayes', 'DM Extra Trees Classifier', 'DM Logistic Regression', 'DM Random Forest Classifier', 'DM K-Nearest Neighbor', 'DM SVM kernel=linear', 'DM SVM kernel=rbf']\n",
    "for file in files:\n",
    "    print(file)\n",
    "    with open ('./data/document_embedding_based_models/' + file + '.pkl', 'rb') as pkl:\n",
    "        model = pickle.load(pkl)\n",
    "    if file.endswith('linear') or file.endswith('rbf'):\n",
    "        y_score = model.decision_function(X_test)\n",
    "    else:\n",
    "        y_score = model.predict_proba(X_test)\n",
    "    # For each class\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    thresh = dict()\n",
    "    f1 = dict()\n",
    "    average_precision = dict()\n",
    "    average_recall = dict()\n",
    "    average_f1_score = dict()\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], thresh[i] = precision_recall_curve(y_test[:, i],\n",
    "                                                            y_score[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n",
    "\n",
    "    # A \"micro-average\": quantifying score on all classes jointly\n",
    "    precision[\"micro\"], recall[\"micro\"], thresh['micro'] = precision_recall_curve(y_test.ravel(),\n",
    "        y_score.ravel())\n",
    "    \n",
    "#     f1['micro'] = 2 * (precision['micro']*recall['micro']) / (precision['micro']+recall['micro'])\n",
    "    \n",
    "    average_precision[\"micro\"] = average_precision_score(y_test, y_score,\n",
    "                                                         average=\"micro\")\n",
    "    print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "          .format(average_precision[\"micro\"]))\n",
    "    \n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (1.5,1.5)    \n",
    "    plt.rcParams['xtick.labelsize']=5\n",
    "    plt.rcParams['ytick.labelsize']=5\n",
    "    plt.step(recall['micro'], precision['micro'], where='post', color='black')\n",
    "    plt.xlabel('Recall', size=5)\n",
    "    plt.ylabel('Precision', size=5)\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title( file, size=5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./data/document_embedding_based_models/'+file+'.tif', dpi=600)\n",
    "    plt.show()\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.step(thresh['micro'], f1['micro'][:-1], where='post')\n",
    "#     plt.xlabel('F1 Score')\n",
    "#     plt.ylabel('Threshold')\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.title('F1-Score, micro-averaged over all classes vs Threshold')\n",
    "#     plt.show()\n",
    "#     print(file + ' Threshold = ', thresh['micro'][np.argmax(f1['micro'][:-1])])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (1.5,1.5)    \n",
    "plt.rcParams['xtick.labelsize']=5\n",
    "plt.rcParams['ytick.labelsize']=5\n",
    "plt.step(recall['micro'], precision['micro'], where='post', color='black')\n",
    "plt.xlabel('Recall', size=5)\n",
    "plt.ylabel('Precision', size=5)\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title( file, size=5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./data/document_embedding_based_models/'+file+'.tif', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "with open ('./data/document_embedding_based_models/label2idx.pkl', 'rb') as pkl:\n",
    "    d = pickle.load(pkl)\n",
    "idx2label = dict(zip(d.values(), d.keys()))\n",
    "\n",
    "files = ['DM SVM kernel=linear', 'DM Gaussian Naive Bayes', 'DM Extra Trees Classifier', 'DM Logistic Regression', 'DM Random Forest Classifier', 'DM K-Nearest Neighbor', 'DM SVM kernel=rbf']\n",
    "thresholds = [0.1511, 0.1842, 0.14, 0.64479, 0.14, 0.4336, 0.5]\n",
    "# files = ['DBOW Logistic Regression']\n",
    "for file, thresh in tqdm_notebook(zip(files, thresholds)):\n",
    "    with open ('./data/document_embedding_based_models/' + file + '.pkl', 'rb') as pkl:\n",
    "        model = pickle.load(pkl)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = [idx2label[np.argmax(pred)] for pred in y_pred]\n",
    "    testing_acc = accuracy_score(y_pred, y_test)\n",
    "    \n",
    "    y_pred = model.predict(X_train)\n",
    "    y_pred = [idx2label[np.argmax(pred)] for pred in y_pred]\n",
    "    training_acc = accuracy_score(y_pred, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    y_pred = [idx2label[np.argmax(pred)] for pred in y_pred]\n",
    "    avg_acc = accuracy_score(y_pred, y)\n",
    "    print('One-vs-All', file, training_acc, testing_acc, avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "with open ('./data/document_embedding_based_models/label2idx.pkl', 'rb') as pkl:\n",
    "    d = pickle.load(pkl)\n",
    "idx2label = dict(zip(d.values(), d.keys()))\n",
    "\n",
    "files = ['DM K-Nearest Neighbor', 'DM SVM kernel=rbf']\n",
    "thresholds = [0.4336, 0.5]\n",
    "# files = ['DBOW Logistic Regression']\n",
    "for file, thresh in tqdm(zip(files, thresholds)):\n",
    "    with open ('./data/document_embedding_based_models/' + file + '.pkl', 'rb') as pkl:\n",
    "        model = pickle.load(pkl)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = [idx2label[np.argmax(pred)] for pred in y_pred]\n",
    "    testing_acc = accuracy_score(y_pred, y_test)\n",
    "    \n",
    "    y_pred = model.predict(X_train)\n",
    "    y_pred = [idx2label[np.argmax(pred)] for pred in y_pred]\n",
    "    training_acc = accuracy_score(y_pred, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    y_pred = [idx2label[np.argmax(pred)] for pred in y_pred]\n",
    "    avg_acc = accuracy_score(y_pred, y)\n",
    "    print('One-vs-All', file, training_acc, testing_acc, avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score, classification_report, accuracy_score\n",
    "import numpy as np\n",
    "x = 'extra_trees_dmm GaussianNB_dmm logreg_dmm svm_dmm random_forest_dmm knn_dmm svm_rbf_dmm'\n",
    "files = x.split(' ')\n",
    "for file in tqdm_notebook(files):\n",
    "    with open ('./data/document_embedding_based_models/' + file + '.pkl', 'rb') as pkl:\n",
    "        model = pickle.load(pkl)\n",
    "    print(file)\n",
    "    \n",
    "    y_preds = model.predict(X_train)\n",
    "    nmi = normalized_mutual_info_score(y_train, y_preds,average_method='arithmetic')\n",
    "    print('Training NMI - ', nmi)\n",
    "    print('Training Accuracy - ', accuracy_score(y_train, y_preds))\n",
    "    \n",
    "    y_preds = model.predict(X_test)\n",
    "    nmi = normalized_mutual_info_score(y_test, y_preds,average_method='arithmetic')\n",
    "    print('Testing NMI - ', nmi)\n",
    "    print('Testing Accuracy - ', accuracy_score(y_test, y_preds))\n",
    "    report = classification_report(y_test, y_preds)\n",
    "    \n",
    "    y_preds = model.predict(X)\n",
    "    nmi = normalized_mutual_info_score(y, y_preds,average_method='arithmetic')\n",
    "    print('Average NMI - ', nmi)\n",
    "    print('Average Accuracy - ', accuracy_score(y, y_preds))\n",
    "        \n",
    "    print(report)\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
